{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4xetVr4z9LISKn7ox0gTg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Torikul385/NLP/blob/main/Text_Classification_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MQ1sgNN9NUL7"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O -q https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njGO7gCWNpUa",
        "outputId": "80bbd5e8-cd51-464a-effe-61bb70c05632"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  17.3M      0  0:00:04  0:00:04 --:--:-- 17.8M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls aclImdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0iXlbhTNyqS",
        "outputId": "1c9de0e3-8b3d-4a04-aebd-22b5bf766c4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls aclImdb/train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRJEIq7nOB3a",
        "outputId": "cbdc704a-a806-4882-c738-d15210c635d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  neg  pos  unsup  unsupBow.feat  urls_neg.txt  urls_pos.txt  urls_unsup.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls aclImdb/test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nxKDds6OFCB",
        "outputId": "aea0b159-3799-4782-cf0c-788805389bea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "jQijD5DdOJbC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_path = 'aclImdb/train'\n",
        "test_ds_path = 'aclImdb/test'\n",
        "\n",
        "train_raw_ds, val_raw_ds = keras.utils.text_dataset_from_directory(\n",
        "    train_ds_path,\n",
        "    subset='both',\n",
        "    validation_split=0.2,\n",
        "    seed=1111\n",
        ")\n",
        "\n",
        "test_raw_ds = keras.utils.text_dataset_from_directory(\n",
        "    test_ds_path,\n",
        "    seed=1111\n",
        ")"
      ],
      "metadata": {
        "id": "3WB6BxDjOSCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9e3439-9252-4ba3-cb0d-448d991ce3f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text, label = next(iter(train_raw_ds))\n",
        "\n",
        "text.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1ruRcaIBvlc",
        "outputId": "91dd4b97-84f3-44a9-83fa-56c83d2c78b2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([32])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = []\n",
        "\n",
        "for text, _ in train_raw_ds.as_numpy_iterator():\n",
        "    for t in text:\n",
        "        t = t.decode('utf-8')\n",
        "        texts.append(t)\n",
        "\n",
        "texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "IwuDf5M6NIe7",
        "outputId": "00028f6f-df22-4de3-fb0d-62c96cf0d4c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Really bad shot on video \"film\" made by not one, not two, but three amateur video makers.<br /><br />If you\\'re going to make a bad horror film at least throw in some blood, gore and nudity. There is some blood provided by latex cut off arm props bought at a Halloween store. There are lesbians and hookers but no nudity or sex. The lesbians spend a lot of time in bed but only talking.<br /><br />There seems to be no editing effects- fades, wipes etc. Once in a while a bit of black appears to separate scenes.<br /><br />Terrible music by bad heavy metal bands whose websites take up the majority of the end credits.The werewolves are represented by rubber masks that are attached to just the \"actors\" face. They didn\\'t even bother to apply brown makeup to their necks, arms or wrists.<br /><br />I guarantee a 10 year old with a video camera could put together a better movie.No reason at all to buy, rent or watch this film except as an example of how not to make a low budget video.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 20000\n",
        "MAXLEN=500\n",
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "EMBEDDING_DIM = 128\n"
      ],
      "metadata": {
        "id": "RcAnIrPb0b7v"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "br = \"<br />\"\n",
        "def customStandard(text):\n",
        "    text = tf.strings.lower(text)\n",
        "    text = tf.strings.regex_replace(text, br, '')\n",
        "    text = tf.strings.regex_replace(text, '[%s]'%re.escape(string.punctuation), '')\n",
        "    return text\n",
        "\n",
        "vectorizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens = VOCAB_SIZE,\n",
        "    standardize = customStandard,\n",
        "    output_sequence_length = MAXLEN\n",
        "\n",
        ")\n",
        "\n",
        "vectorizer.adapt(texts)"
      ],
      "metadata": {
        "id": "DoZ_mHLl3Keo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess(text, labels):\n",
        "    #text = tf.expand_dims(text, axis=-1)\n",
        "    text = vectorizer(text)\n",
        "    return text, labels\n",
        "\n",
        "def prepare_ds(ds):\n",
        "    ds = ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.prefetch(buffer_size = 10*BATCH_SIZE).cache()\n",
        "    return ds\n",
        "\n",
        "train_ds = prepare_ds(train_raw_ds)\n",
        "val_ds = prepare_ds(val_raw_ds)\n",
        "test_ds = prepare_ds(test_raw_ds)"
      ],
      "metadata": {
        "id": "MvUk61_NKf0S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text, labels = next(iter(train_ds))\n",
        "text.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZPTmeT25AxR",
        "outputId": "45469a31-74c4-4f1e-e533-5564a7605255"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([32, 500])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "b0kdiaHx8egS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_ds, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRZxw9RSBUN9",
        "outputId": "bb04ca92-2767-420d-fce8-c23533e63d2c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "625/625 [==============================] - 139s 216ms/step - loss: 0.5436 - accuracy: 0.6725\n",
            "Epoch 2/3\n",
            "625/625 [==============================] - 129s 207ms/step - loss: 0.2481 - accuracy: 0.9008\n",
            "Epoch 3/3\n",
            "625/625 [==============================] - 128s 205ms/step - loss: 0.1248 - accuracy: 0.9549\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d1d93b2e920>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PpAF2p5BCddb"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}