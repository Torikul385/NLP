{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q --upgrade keras-nlp \n!pip install -q --upgrade keras\n!pip install -q --upgrade rouge-score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-10T11:51:08.461803Z","iopub.execute_input":"2024-07-10T11:51:08.462082Z","iopub.status.idle":"2024-07-10T11:51:54.485236Z","shell.execute_reply.started":"2024-07-10T11:51:08.462057Z","shell.execute_reply":"2024-07-10T11:51:54.484102Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import keras_nlp\nimport pathlib\nimport random\n\nimport keras\nfrom keras import ops\n\nimport tensorflow.data as tf_data\nfrom tensorflow_text.tools.wordpiece_vocab import (\n    bert_vocab_from_dataset as bert_vocab,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:51:54.487626Z","iopub.execute_input":"2024-07-10T11:51:54.488450Z","iopub.status.idle":"2024-07-10T11:52:08.538573Z","shell.execute_reply.started":"2024-07-10T11:51:54.488404Z","shell.execute_reply":"2024-07-10T11:52:08.537589Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-10 11:51:56.342067: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-10 11:51:56.342172: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-10 11:51:56.481718: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"BATCH_SIZE = 64\nEPOCHS = 1  # This should be at least 10 for convergence\nMAX_SEQUENCE_LENGTH = 40\nENG_VOCAB_SIZE = 15000\nSPA_VOCAB_SIZE = 15000\n\nEMBED_DIM = 256\nINTERMEDIATE_DIM = 2048\nNUM_HEADS = 8\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:52:08.539707Z","iopub.execute_input":"2024-07-10T11:52:08.540194Z","iopub.status.idle":"2024-07-10T11:52:08.544842Z","shell.execute_reply.started":"2024-07-10T11:52:08.540170Z","shell.execute_reply":"2024-07-10T11:52:08.543903Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"text_file = keras.utils.get_file(\n    fname=\"spa-eng.zip\",\n    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n    extract=True,\n)\ntext_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:52:08.546713Z","iopub.execute_input":"2024-07-10T11:52:08.547022Z","iopub.status.idle":"2024-07-10T11:52:08.692615Z","shell.execute_reply.started":"2024-07-10T11:52:08.546998Z","shell.execute_reply":"2024-07-10T11:52:08.691942Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n\u001b[1m2638744/2638744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"with open(text_file) as f:\n    lines = f.read().split(\"\\n\")[:-1]\ntext_pairs = []\nfor line in lines:\n    eng, spa = line.split(\"\\t\")\n    eng = eng.lower()\n    spa = spa.lower()\n    text_pairs.append((eng, spa))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:52:08.693614Z","iopub.execute_input":"2024-07-10T11:52:08.693905Z","iopub.status.idle":"2024-07-10T11:52:08.941609Z","shell.execute_reply.started":"2024-07-10T11:52:08.693881Z","shell.execute_reply":"2024-07-10T11:52:08.940834Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"for _ in range(5):\n    print(random.choice(text_pairs))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:52:08.942753Z","iopub.execute_input":"2024-07-10T11:52:08.943095Z","iopub.status.idle":"2024-07-10T11:52:08.948108Z","shell.execute_reply.started":"2024-07-10T11:52:08.943064Z","shell.execute_reply":"2024-07-10T11:52:08.947231Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"('tom likes to listen to mary sing.', 'a tom le gusta escuchar a mary cantar.')\n(\"what's the tallest mountain you've climbed?\", '¿cuál es la montaña más alta que has escalado?')\n(\"i can't buy you that dress.\", 'no puedo comprarte ese vestido.')\n('the man stood up.', 'el hombre se puso de pie.')\n('i really like china.', 'realmente me gusta china.')\n","output_type":"stream"}]},{"cell_type":"code","source":"random.shuffle(text_pairs)\nnum_val_samples = int(0.15 * len(text_pairs))\nnum_train_samples = len(text_pairs) - 2 * num_val_samples\ntrain_pairs = text_pairs[:num_train_samples]\nval_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\ntest_pairs = text_pairs[num_train_samples + num_val_samples :]\n\nprint(f\"{len(text_pairs)} total pairs\")\nprint(f\"{len(train_pairs)} training pairs\")\nprint(f\"{len(val_pairs)} validation pairs\")\nprint(f\"{len(test_pairs)} test pairs\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:52:08.949409Z","iopub.execute_input":"2024-07-10T11:52:08.949940Z","iopub.status.idle":"2024-07-10T11:52:09.059335Z","shell.execute_reply.started":"2024-07-10T11:52:08.949910Z","shell.execute_reply":"2024-07-10T11:52:09.058515Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"118964 total pairs\n83276 training pairs\n17844 validation pairs\n17844 test pairs\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_word_piece(text_samples, vocab_size, reserved_tokens):\n    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n        word_piece_ds.batch(1000).prefetch(2),\n        vocabulary_size=vocab_size,\n        reserved_tokens=reserved_tokens,\n    )\n    return vocab\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:52:09.060306Z","iopub.execute_input":"2024-07-10T11:52:09.060582Z","iopub.status.idle":"2024-07-10T11:52:09.065396Z","shell.execute_reply.started":"2024-07-10T11:52:09.060559Z","shell.execute_reply":"2024-07-10T11:52:09.064524Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n\neng_samples = [text_pair[0] for text_pair in train_pairs]\neng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n\nspa_samples = [text_pair[1] for text_pair in train_pairs]\nspa_vocab = train_word_piece(spa_samples, SPA_VOCAB_SIZE, reserved_tokens)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:52:09.066470Z","iopub.execute_input":"2024-07-10T11:52:09.066817Z","iopub.status.idle":"2024-07-10T11:54:22.303826Z","shell.execute_reply.started":"2024-07-10T11:52:09.066787Z","shell.execute_reply":"2024-07-10T11:54:22.302976Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(\"English Tokens: \", eng_vocab[100:110])\nprint(\"Spanish Tokens: \", spa_vocab[100:110])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:54:22.306696Z","iopub.execute_input":"2024-07-10T11:54:22.306987Z","iopub.status.idle":"2024-07-10T11:54:22.311886Z","shell.execute_reply.started":"2024-07-10T11:54:22.306962Z","shell.execute_reply":"2024-07-10T11:54:22.311006Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"English Tokens:  ['him', 'there', 'they', 'go', 'her', 'has', 'will', 're', 'll', 'time']\nSpanish Tokens:  ['te', 'para', 'mary', 'las', 'más', 'al', 'yo', 'tu', 'estoy', 'muy']\n","output_type":"stream"}]},{"cell_type":"code","source":"eng_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n    vocabulary=eng_vocab, lowercase=False\n)\nspa_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n    vocabulary=spa_vocab, lowercase=False\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:54:22.312982Z","iopub.execute_input":"2024-07-10T11:54:22.313317Z","iopub.status.idle":"2024-07-10T11:54:22.403430Z","shell.execute_reply.started":"2024-07-10T11:54:22.313285Z","shell.execute_reply":"2024-07-10T11:54:22.402704Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"eng_input_ex = text_pairs[0][0]\neng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\nprint(\"English sentence: \", eng_input_ex)\nprint(\"Tokens: \", eng_tokens_ex)\nprint(\n    \"Recovered text after detokenizing: \",\n    eng_tokenizer.detokenize(eng_tokens_ex),\n)\n\nprint()\n\nspa_input_ex = text_pairs[0][1]\nspa_tokens_ex = spa_tokenizer.tokenize(spa_input_ex)\nprint(\"Spanish sentence: \", spa_input_ex)\nprint(\"Tokens: \", spa_tokens_ex)\nprint(\n    \"Recovered text after detokenizing: \",\n    spa_tokenizer.detokenize(spa_tokens_ex),\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:54:22.404828Z","iopub.execute_input":"2024-07-10T11:54:22.405237Z","iopub.status.idle":"2024-07-10T11:54:22.528903Z","shell.execute_reply.started":"2024-07-10T11:54:22.405209Z","shell.execute_reply":"2024-07-10T11:54:22.527966Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"English sentence:  you're not allowed in there.\nTokens:  tf.Tensor([  66    8  107   95 1258   71  101   12], shape=(8,), dtype=int32)\nRecovered text after detokenizing:  tf.Tensor(b\"you ' re not allowed in there .\", shape=(), dtype=string)\n\nSpanish sentence:  no estás permitida allá adentro.\nTokens:  tf.Tensor([  78  155   45 1443 2403  843 3691  655 1775   15], shape=(10,), dtype=int32)\nRecovered text after detokenizing:  tf.Tensor(b'no est\\xc3\\xa1s permitida all\\xc3\\xa1 adentro .', shape=(), dtype=string)\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_batch(eng, spa):\n    batch_size = ops.shape(spa)[0]\n\n    eng = eng_tokenizer(eng)\n    spa = spa_tokenizer(spa)\n\n    # Pad `eng` to `MAX_SEQUENCE_LENGTH`.\n    eng_start_end_packer = keras_nlp.layers.StartEndPacker(\n        sequence_length=MAX_SEQUENCE_LENGTH,\n        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n    )\n    eng = eng_start_end_packer(eng)\n\n    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `spa` and pad it as well.\n    spa_start_end_packer = keras_nlp.layers.StartEndPacker(\n        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n        start_value=spa_tokenizer.token_to_id(\"[START]\"),\n        end_value=spa_tokenizer.token_to_id(\"[END]\"),\n        pad_value=spa_tokenizer.token_to_id(\"[PAD]\"),\n    )\n    spa = spa_start_end_packer(spa)\n\n    return (\n        {\n            \"encoder_inputs\": eng,\n            \"decoder_inputs\": spa[:, :-1],\n        },\n        spa[:, 1:],\n    )\n\n\ndef make_dataset(pairs):\n    eng_texts, spa_texts = zip(*pairs)\n    eng_texts = list(eng_texts)\n    spa_texts = list(spa_texts)\n    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n    return dataset.shuffle(2048).prefetch(16).cache()\n\n\ntrain_ds = make_dataset(train_pairs)\nval_ds = make_dataset(val_pairs)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:54:22.530116Z","iopub.execute_input":"2024-07-10T11:54:22.530523Z","iopub.status.idle":"2024-07-10T11:54:25.462327Z","shell.execute_reply.started":"2024-07-10T11:54:22.530487Z","shell.execute_reply":"2024-07-10T11:54:25.461526Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for inputs, targets in train_ds.take(1):\n    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n    print(f\"targets.shape: {targets.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:54:25.463469Z","iopub.execute_input":"2024-07-10T11:54:25.463777Z","iopub.status.idle":"2024-07-10T11:54:26.578861Z","shell.execute_reply.started":"2024-07-10T11:54:25.463751Z","shell.execute_reply":"2024-07-10T11:54:26.577793Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"inputs[\"encoder_inputs\"].shape: (64, 40)\ninputs[\"decoder_inputs\"].shape: (64, 40)\ntargets.shape: (64, 40)\n","output_type":"stream"}]},{"cell_type":"code","source":"encoder_inputs = keras.Input(shape=(None,), name=\"encoder_inputs\")\n\nx = keras_nlp.layers.TokenAndPositionEmbedding(\n    vocabulary_size=ENG_VOCAB_SIZE,\n    sequence_length=MAX_SEQUENCE_LENGTH,\n    embedding_dim=EMBED_DIM,\n)(encoder_inputs)\n\nencoder_outputs = keras_nlp.layers.TransformerEncoder(\n    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n)(inputs=x)\nencoder = keras.Model(encoder_inputs, encoder_outputs)\n\n\n# Decoder\ndecoder_inputs = keras.Input(shape=(None,), name=\"decoder_inputs\")\nencoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n\nx = keras_nlp.layers.TokenAndPositionEmbedding(\n    vocabulary_size=SPA_VOCAB_SIZE,\n    sequence_length=MAX_SEQUENCE_LENGTH,\n    embedding_dim=EMBED_DIM,\n)(decoder_inputs)\n\nx = keras_nlp.layers.TransformerDecoder(\n    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n)(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\nx = keras.layers.Dropout(0.5)(x)\ndecoder_outputs = keras.layers.Dense(SPA_VOCAB_SIZE, activation=\"softmax\")(x)\ndecoder = keras.Model(\n    [\n        decoder_inputs,\n        encoded_seq_inputs,\n    ],\n    decoder_outputs,\n)\ndecoder_outputs = decoder([decoder_inputs, encoder_outputs])\n\ntransformer = keras.Model(\n    [encoder_inputs, decoder_inputs],\n    decoder_outputs,\n    name=\"transformer\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:54:26.580901Z","iopub.execute_input":"2024-07-10T11:54:26.581530Z","iopub.status.idle":"2024-07-10T11:54:26.794836Z","shell.execute_reply.started":"2024-07-10T11:54:26.581494Z","shell.execute_reply":"2024-07-10T11:54:26.794055Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"transformer.summary()\ntransformer.compile(\n    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:54:26.796066Z","iopub.execute_input":"2024-07-10T11:54:26.796380Z","iopub.status.idle":"2024-07-10T11:54:26.830449Z","shell.execute_reply.started":"2024-07-10T11:54:26.796355Z","shell.execute_reply":"2024-07-10T11:54:26.829783Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"transformer\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_and_position… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m3,850,240\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mTokenAndPositionE…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_encoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m1,315,072\u001b[0m │ token_and_positi… │\n│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ functional_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m9,283,992\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m15000\u001b[0m)            │            │ transformer_enco… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_and_position… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,850,240</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionE…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,072</span> │ token_and_positi… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ functional_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">9,283,992</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)            │            │ transformer_enco… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,449,304\u001b[0m (55.12 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,449,304</span> (55.12 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,449,304\u001b[0m (55.12 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,449,304</span> (55.12 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"history = transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:54:26.831599Z","iopub.execute_input":"2024-07-10T11:54:26.832041Z","iopub.status.idle":"2024-07-10T11:56:13.917605Z","shell.execute_reply.started":"2024-07-10T11:54:26.832009Z","shell.execute_reply":"2024-07-10T11:56:13.916687Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\u001b[1m   2/1302\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 80ms/step - accuracy: 0.1917 - loss: 9.1808      ","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1720612483.477658     128 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1720612483.543647     128 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1720612483.558213     128 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 951/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 66ms/step - accuracy: 0.8024 - loss: 1.6543","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1720612546.328321     127 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 69ms/step - accuracy: 0.8233 - loss: 1.4459 - val_accuracy: 0.9809 - val_loss: 0.1613\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1720612573.721125     127 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"}]},{"cell_type":"code","source":"def decode_sequences(input_sentences):\n    batch_size = 1\n\n    # Tokenize the encoder input.\n    encoder_input_tokens = ops.convert_to_tensor(eng_tokenizer(input_sentences))\n    if len(encoder_input_tokens[0]) < MAX_SEQUENCE_LENGTH:\n        pads = ops.full((1, MAX_SEQUENCE_LENGTH - len(encoder_input_tokens[0])), 0)\n        encoder_input_tokens = ops.concatenate(\n            [encoder_input_tokens.to_tensor(), pads], 1\n        )\n\n    # Define a function that outputs the next token's probability given the\n    # input sequence.\n    def next(prompt, cache, index):\n        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n        # Ignore hidden states for now; only needed for contrastive search.\n        hidden_states = None\n        return logits, hidden_states, cache\n\n    # Build a prompt of length 40 with a start token and padding tokens.\n    length = 40\n    start = ops.full((batch_size, 1), spa_tokenizer.token_to_id(\"[START]\"))\n    pad = ops.full((batch_size, length - 1), spa_tokenizer.token_to_id(\"[PAD]\"))\n    prompt = ops.concatenate((start, pad), axis=-1)\n\n    generated_tokens = keras_nlp.samplers.GreedySampler()(\n        next,\n        prompt,\n        stop_token_ids=[spa_tokenizer.token_to_id(\"[END]\")],\n        index=1,  # Start sampling after start token.\n    )\n    generated_sentences = spa_tokenizer.detokenize(generated_tokens)\n    return generated_sentences\n\n\ntest_eng_texts = [pair[0] for pair in test_pairs]\nfor i in range(2):\n    input_sentence = random.choice(test_eng_texts)\n    translated = decode_sequences([input_sentence])\n    translated = translated.numpy()[0].decode(\"utf-8\")\n    translated = (\n        translated.replace(\"[PAD]\", \"\")\n        .replace(\"[START]\", \"\")\n        .replace(\"[END]\", \"\")\n        .strip()\n    )\n    print(f\"** Example {i} **\")\n    print(input_sentence)\n    print(translated)\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:56:13.918907Z","iopub.execute_input":"2024-07-10T11:56:13.919199Z","iopub.status.idle":"2024-07-10T11:56:21.820951Z","shell.execute_reply.started":"2024-07-10T11:56:13.919172Z","shell.execute_reply":"2024-07-10T11:56:21.820152Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"** Example 0 **\ncome as soon as possible.\nestaba realmente estaba parecía estúpido\n\n** Example 1 **\ni've got cookies.\nntar hacer sus sucedió estúpido\n\n","output_type":"stream"}]}]}